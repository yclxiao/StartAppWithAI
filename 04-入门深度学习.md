## 1、深度学习概述

### 1.1、什么是深度学习

我们知道，深度学习是机器学习的一个子集，有了机器学习为什么又需要深度学习？深度学习又是什么？

深度学习主要基于人工神经网络的发展而来，其关键在于“深度”，即神经网络中的层数。随着层数的增加，网络能够学习到更加抽象和高级的特征，从而在复杂任务中表现出色。

现代的深度学习模型，如卷积神经网络（CNN）和循环神经网络（RNN）、Transformer架构等，已经在很多实际应用中取得了瞩目的成就。

![](https://img.mangoant.top/blog/202408281650433.png)



### 1.2、从机器学习到深度学习

我们再回顾下机器学习的定义：***让机器从数据集中学习，找到合适的函数，再用这个函数做预测。***这里最核心的就**函数**。

**函数**是一种确定性的映射关系，在函数里，对于给定的输入，总能得到确定性的输出。

那在面对更加复杂的数据关系时，我们则需要更加复杂的函数来表示，比如说神经网络。而深度学习便是聚焦于深度神经网络模型的一个机器学习子领域。

深度学习说白了就是一种使用多层神经网络算法的机器学习模型，利用多层神经网络算法从数据集中学习，然后提取特征。

这里的神经网络和我们的大脑神经，只是名字相同，其实没啥关系，所以忘掉大脑神经的概念吧。

如下图所示，是一个简单的神经网络结构。左侧方框表示输入，圆圈表示神经元，右侧表示输出。除了最左侧、最右侧，中间都是隐藏层。每个神经元都表示一个函数。

神经网络使用分而治之的策略来学习，网络中的每个神经元学习一个简单函数，而这些简单函数的组合就构成了复杂的神经网络。

![](https://img.mangoant.top/blog/202408301347946.png)



这么说可能还是有点抽象，让我们再次简化下神经网络的概念。

首先，不管任何数据到了计算机里，都是数字。那所谓的机器学习，其实就是给定机器一些带参数的公式，让机器通过这些数字规律，去确定公式里的参数，从而最终确定出公式。

其次就是每个神经元都是一个数据公式，每个神经元都会参与数字的运算，神经元的作用说白了就是对数字的变换或者处理。通过大量的神经元去处理数字，最终会沉淀出数字的规则。这个规则就是模型。

所以说，神经网络的每一层都相当于一个助手，每一层都会用来变换数据。神经网络就是通过一些数学公式去变换数据。神经网络里，只有数据和权重、矩阵 计算的过程而已。

### 1.3、为什么需要深度学习

传统的机器学习需要先做各种各样的特征工程，让数据变得“计算机友好”，再输入模型进行学习。机器学习的数据集，具有良好的结构，每一个特征是什么，几乎都能说清楚。

尽管机器学习在许多应用中表现良好，比如推荐系统、部分预测系统。但在处理复杂的非结构化数据（比如图像和语音）时，传统的机器学习往往捉襟见肘，因为这些数据集的特征长什么样呢？不太容易说清楚。

而深度学习模型则可以自动进行特征提取，因此就省略掉了手工做特征工程的环节。你可以看一看图中所示的这个图片识别问题的机器学习流程。

![](https://secure2.wostatic.cn/static/rh5AQJsiqbEutXH6ot6ixV/image.png?auth_key=1728974271-xqfYvAgvNcFsVQ179KTVRC-0-d2ba7b9f145336be16cd2e735c35994b)

深层神经网络的厉害之处在于，它能对非结构的数据集进行自动的复杂特征提取，不需要人工干预。也就是说，深度学习让之前需要大量人工标注数据的这个“难题”变得非常容易。

所以，与传统的机器学习方法相比，深度学习的独特之处在于其使用多层神经网络，这些网络具有更强的自动学习和特征提取能力。这不仅提高了模型的准确性，也减少了对人工标注的依赖。

深度学习特别擅长处理高维度和大规模的非结构化数据，比如图像、语音、文本等。尤其是当数据量庞大且特征复杂时，深度学习的神经网络模型可以通过层层抽象，自动从数据中学习特征，逐步从低级特征（如边缘、纹理）提取到高级特征（如面部、物体）。

## 2、深度学习的历史

### 2.1、人工神经网络的起源

深度学习的起源可以追溯到20世纪40年代，当时的科学家们从人脑神经元结构得到启发，开始探索神经网络的计算模型。这些早期的研究奠定了神经网络的基础。

**感知器**是最早的神经网络模型之一，感知器是一个简单的线性分类器，可以对输入数据进行二分类。尽管感知器模型在处理简单的任务时表现不错，但它无法处理非线性问题，这使得它在复杂的任务中表现不佳。

### 2.2、深度学习的发展历程

随着计算能力的提升和算法的改进，神经网络逐渐发展出更多层次，并演变成深度神经网络（DNNs）。

1986年，**反向传播算法**（Backpropagation）的提出是神经网络发展的重要里程碑。这种算法通过计算损失函数相对于每一层权重的梯度来更新网络参数，从而使得多层神经网络的训练成为可能。

尽管反向传播算法推动了神经网络的发展，但由于计算资源的限制和训练深度网络的困难，神经网络在20世纪90年代至21世纪初一度陷入低谷。这段时期被称为“AI寒冬期”，许多研究者对神经网络失去信心。

### 2.3、现代深度学习的里程碑

2012年是深度学习的爆发元年。由杰弗里·辛顿教授和他的2位学生Alex Krizhevsky和Ilya Sutskever开发的**AlexNet**在ImageNet图像识别竞赛中大获成功，标志着深度学习重新回到研究的前沿。

AlexNet是一个由8层网络组成的卷积神经网络（CNN），它大幅度提升了图像分类的准确率，当时的比赛中，**AlexNet**的成绩遥遥领先于第二名。

在接下来的几年中，深度学习迅速发展，出现了许多新的模型和架构。这些模型通过增加网络的深度和改进结构，进一步提高了图像识别的性能。

在自然语言处理（NLP）领域，2017年由Vaswani等人提出的**Transformer**架构也是一个重要的里程碑。

Transformer通过引入自注意力机制，解决了传统RNN在处理长序列数据时的局限性，极大地提升了机器翻译和文本生成的效果。



## 3、深度学习里的核心概念

随着我们继续入门深度学习，有一些必要的核心概念需要了解，具体如下。

### 3.1、感知机

在深度学习中，感知机是最简单的一种人工神经网络形式，它可以被视为是现代复杂神经网络的基础构建块。它是一个二元线性分类器。

感知机由一个或多个输入节点（对应于输入特征）、一个权重（Weight）向量、一个偏置（Bias）以及一个输出节点组成。每个输入特征都与权重相乘，然后将所有的乘积之和加上偏置，这个结果被传递给一个激活函数，最终输出一个二元分类结果（例如0或1）。

感知机只能解决线性可分问题，尽管它本身较为简单，但它的概念被广泛应用和扩展到更复杂的网络结构中，比如多层感知机、卷积神经网络（CNN）和递归神经网络（RNN）等。

感知机是现代深度学习模型的基础之一。

### 3.2、神经网络的组成

神经网络是深度学习的核心，它由许多相互连接的节点（称为神经元）组成。一个典型的神经网络包括三个主要部分：输入层、隐藏层和输出层。

- **输入层**：接收数据输入，每个神经元代表输入数据的一个特征。
- **隐藏层**：处理数据并提取特征，每一层通过权重和偏置对输入进行变换，再通过激活函数引入非线性，使得神经网络能够学习到复杂的特征。
- **输出层**：产生最终的输出结果，用于分类或回归等任务。

我们要想神经网络的效果更好，要不就是增加某一层的神经元，要不就是增加隐藏层数量。

### 3.3、激活函数

激活函数是神经网络中引入非线性能力的关键组件。它决定了一个神经元是否被激活，影响数据在网络中的传播。常见的激活函数包括：

- **ReLU**：ReLU是目前使用最广泛的激活函数之一，它将输入的负值归零，仅保留正值。ReLU的计算简单且能够有效缓解梯度消失问题。
- **Sigmoid**：将输入映射到(0, 1)区间，常用于二分类问题的输出层。
- **Tanh**：类似于Sigmoid，但将输入映射到(-1, 1)区间，常用于RNN中。

### 3.4、损失函数

损失函数是用来衡量模型的预测结果与实际结果之间的差距。通过最小化损失函数，模型的性能得以提升。常见的损失函数包括：

- **均方误差（MSE）**：常用于回归问题，计算预测值与实际值之间的平方差。
- **交叉熵损失（Cross-Entropy Loss）**：常用于分类问题，衡量两个概率分布之间的差异。

### 3.5、优化器

优化器是用来更新神经网络权重的算法，它基于损失函数的梯度来调整网络的参数。优化器的选择对模型的训练速度和最终性能有重要影响。常见的优化器包括：

- **梯度下降法（SGD）**：通过计算损失函数对每个参数的导数，沿着梯度的反方向更新参数。
- **Adam**：结合了动量和自适应学习率的优化方法，通常能在较少的训练迭代中获得较好的结果。

### 3.6、前向传播与反向传播

前向传播和反向传播是神经网络训练过程的两个重要步骤。

- **前向传播**：输入数据经过各层网络处理，最终得到预测结果。在此过程中，各层的输出通过激活函数传递给下一层。
- **反向传播**：根据损失函数计算出的误差，使用高等数学里的链式法则从输出层逐层向回传播梯度。然后，通过优化器调整各层的权重，以减少预测误差。

前向传播负责生成预测结果，而反向传播则通过计算误差并调整每一层的权重参数来优化模型。这两者的配合才使得神经网络的模型能够逐步最优。

### 3.7、如何组合出复杂模型

在电路中，可以通过一个并联、一个串联，组合出第三种电路。同样的，在函数中也可以通过2个简单的函数组合出第三个函数。

在比如说大规模集成电路也是由一个个简单的元器件和开关组合而成。比如二极管、电阻、晶体管、变压器等，每个元器件都是简单的功能，随着组合的元器件数量和开关数量的剧增，就形成了大规模集成电路。

这种通过组合较小、较简单的模型，构建复杂模型的策略就是神经网络的核心思想。

### 3.8、神经元如何处理信息

神经元实现的也是将多个输入映射为一个输出的两阶段信息处理方法。

信息处理的第一阶段计算神经元输入值的加权和。第二阶段将加权和输入一个函数，该函数将加权和映射为神经元的最终输出值。

在设计神经元时，我们可以使用很多不同类型的函数作为第二阶段的信息处理函数。神经元的输出值常被称为激活值。因此，将加权和映射为神经元激活值的函数也被称为激活函数。

![](https://img.mangoant.top/blog/202408301418982.png)

神经元信息处理的第二阶段将加权和结果，即z值，传给激活函数。由激活函数的结果决定是否传递给下一个神经元。

常见的激活函数有：ReLU函数、阈值函数、Sigmoid函数、双曲正切函数。

### 3.9、为什么需要激活函数

我们的现实世界是非常复杂的，无法用线性关系去表示。如果神经网络在加权计算后，得出的就是线性函数，很难表示我们的复杂世界。

因此，为了使神经网络能够实现非线性映射，神经元在处理信息时就要包含非线性步骤（即激活函数）​。神经网络的层数越深，学习复杂非线性映射的能力就越强。



### 3.10、使用GPU可以加速神经网络的训练

神经网络的训练过程涉及大量的矩阵乘法和加法运算，特别是在前向传播和反向传播的过程中。**GPU**擅长大规模矩阵运算，从而加速整个计算过程。

**GPU**（图形处理单元）设计之初是为了处理图像和视频中的大量像素计算，这类计算涉及大量的矩阵操作和向量计算。这种计算与神经网络训练中的矩阵运算非常相似。

**GPU的架构**包含成千上万个小型核心，可以同时处理多个操作。这使得它能够在同一时间并行处理大量的神经元更新。

GPU通常具有更高的内存带宽，可以更快地从显存中读取和写入数据。这对大规模训练深度学习模型就太重要。

现代的深度学习框架（如TensorFlow、PyTorch）都对GPU提供了很好的支持，并且通过使用英伟达的CUDA等库，可以进一步压榨GPU的性能，充分发挥GPU的计算能力。



## 4、深度学习的核心架构

### 4.1、前馈神经网络（Feedforward Neural Networks, FNNs）

前馈神经网络是最基础的神经网络架构，数据从输入层开始，一层一层向前传播，最终在输出层生成结果。在前馈网络中，信息只向前传递，不存在反馈连接。

前馈网络通常用于结构化数据的回归或分类任务。尽管其结构简单，但在处理复杂数据时，性能往往不如其他更复杂的架构。

### 4.2、卷积神经网络（Convolutional Neural Networks, CNNs）

卷积神经网络（CNN）是专门用于处理图像数据的深度学习架构。CNN通过引入卷积层来提取图像中的空间特征，在图像识别、目标检测等领域表现优异。

- **卷积层（Convolutional Layer）**：卷积层使用卷积核（filter）在输入图像上进行滑动，生成特征图（feature map）。这些卷积核能够捕捉到图像中的局部模式，如边缘、角点等。
- **池化层（Pooling Layer）**：池化层通常跟随在卷积层之后，用于降低特征图的尺寸，从而减少参数数量和计算量。常见的池化方法包括最大池化（max pooling）和平均池化（average pooling）。
- **全连接层（Fully Connected Layer）**：在经过多个卷积和池化层后，特征图被展平并传递到全连接层，全连接层负责将提取的特征映射到最终的输出类别。

CNN通过层级结构逐步提取图像中的特征，最终用于图像分类、物体检测等任务。

### 4.3、循环神经网络（Recurrent Neural Networks, RNNs）

循环神经网络（RNN）是一种用于处理序列数据的神经网络架构，特别适合时间序列分析、自然语言处理等任务。RNN的特点在于其“记忆”能力，即它可以将前一步的输出作为下一步的输入，因此能够捕捉序列数据中的依赖关系。

- **标准RNN**：标准的RNN结构简单，但在处理长序列时容易出现梯度消失问题，导致早期输入的信息难以传递到后期。
- **长短期记忆网络（LSTM）**：为了解决RNN的梯度消失问题，LSTM通过引入记忆单元和门控机制，来控制信息的流动，从而保留长期依赖信息。

RNN和其变体广泛应用于语音识别、机器翻译、文本生成等任务。通过其递归结构，RNN能够捕捉输入序列中的时间依赖性。

### 4.4、生成对抗网络（Generative Adversarial Networks, GANs）

生成对抗网络（GANs）是一种用于生成数据的深度学习架构。GAN由两个网络组成：生成器（Generator）和判别器（Discriminator）。

- **生成器**：生成器的任务是生成假数据，希望能欺骗判别器，使其认为这些假数据是真实的。
- **判别器**：判别器的任务是区分生成的数据与真实数据。

GAN通过生成器和判别器之间的对抗性训练，使生成器逐渐学习到生成逼真数据的能力。GAN在图像生成、图像修复、风格迁移等领域展现了强大的能力。比如，使用GAN可以生成高质量的虚拟人脸、增强图像的分辨率等。

### 4.5、自编码器（Autoencoders）

自编码器是一种用于无监督学习的神经网络，用于数据的降维、特征学习和生成模型。自编码器由编码器（Encoder）和解码器（Decoder）组成。

- **编码器**：编码器将输入数据压缩到低维空间，提取关键特征。
- **解码器**：解码器则将压缩后的特征重构为原始数据，尽量使重构数据与原始数据接近。

自编码器可以用于降噪、数据压缩、特征提取等任务。

### 4.6、Transformer架构

Transformer是一种基于注意力机制的神经网络架构，最初由Vaswani等人在2017年提出。Transformer在自然语言处理（NLP）中表现出色，成为机器翻译、文本生成等任务的主流架构。

- **自注意力机制（Self-Attention）**：Transformer通过引入自注意力机制来捕捉输入序列中各个元素之间的关系，无需像RNN那样依赖序列顺序。自注意力机制能够并行处理数据，提高了模型的效率。
- **编码器-解码器结构**：Transformer由编码器（Encoder）和解码器（Decoder）组成，编码器负责将输入序列转换为上下文表示，解码器则基于这些表示生成输出序列。
- **应用与发展**：Transformer模型在NLP任务中取得了显著成果，比如BERT、GPT等均使用Transformer架构。BERT通过双向编码器预训练，增强了模型的上下文理解能力，而GPT则在文本生成任务中表现优秀。



## 5、深度学习的基本步骤

深度学习与机器学习有个很大的区别，机器学习需要人工标注特征，深度学习可以自己提取特征，深度学习框架会自己学习，无需人工干预。

但深度学习的步骤和机器学习基本相似，只是在选择算法上有些不同，深度学习使用神经网络算法。

### 5.1、核心步骤

1. 数据收集和预处理
2. 构建特征集和标签集
3. 特征工程和数据集拆分
4. 选择算法建立模型
    1. 选择算法
    2. 建立模型
5. 模型的训练和拟合
6. 模型性能的评估



### 5.2、选择算法

深层神经网络是由大量的神经元联结而成，每个神经元都具有可以调整的参数，而训练机器、建立模型的过程，也就是确定网络中神经元参数和权重参数的过程。一旦参数确定，模型也就确定下来了。关于深度神经网络的结构，你可以看看下面的图。

![](https://secure2.wostatic.cn/static/gXNBwXNfRHzDxrh9kYZsRL/image.png)



### 5.3、构建模型

建立神经网络模型的步骤，可以按照输入层、隐藏层、输出层来划分。

隐藏层里的每一层都需要类型、激活函数。简单示例如下：

```Python
model = models.Sequential()
model.add(layers.Dense(128, activation='relu', input_shape=(784,)))  # 输入层，假设输入维度为784
model.add(layers.Dense(64, activation='relu'))  # 隐藏层
model.add(layers.Dense(10, activation='softmax'))  # 输出层，假设有10个类别



```



### 5.4、工具和框架

现代深度学习的发展得益于强大的框架支持，这些框架使得开发者能够方便地构建、训练和部署复杂的神经网络模型。以下是几个主流的深度学习框架：

- **TensorFlow**：由Google开发的开源深度学习框架。TensorFlow提供了广泛的工具和库，支持大规模分布式训练，并且可以在各种平台上部署模型。其灵活性使得它在研究和工业应用中广受欢迎。
- **PyTorch**：由Facebook的AI Research团队开发。PyTorch以其易用性和动态计算图的特性受到研究人员的青睐。PyTorch在学术界和工业界都有广泛应用，特别是在研究和实验阶段。
- **Keras**：最初是一个独立的高层API，后来成为TensorFlow的一部分。Keras的设计目标是简化神经网络的构建过程，具有快速原型开发的特点。Keras 把 TensorFlow 的底层深度学习功能进行了良好的封装，是最好用、最适合初学者上手的深度学习工具包。所以，我们下面就选择 Keras 来搭建神经网络。



## 6、案例

以上内容主要梳理了深度学习的脉络，下面通过实操几个案例加深对深度学习的理解。

### 6.1、观看神经网络训练过程

进入如下网站，观察一个分类任务的神经网络训练过程。读者可以自行操作，选择不同的数据复杂度，调整神经元数量、调整神经网络层数、设置激活函数、设置正则化等。

[https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=6,5,2&seed=0.36641&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=true&ySquared=true&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=6,5,2&seed=0.36641&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=true&ySquared=true&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)

![](https://img.mangoant.top/blog/202409120926800.png)

### 6.2、识别手写数字

识别手写数字案例，具体代码如下：

```Python
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
import numpy as np

#从本地加载MNIST数据集
data_path = '/Users/yclxiao/Downloads/mnist.npz'
with np.load(data_path) as data:
    x_train, y_train = data['x_train'], data['y_train']
    x_test, y_test = data['x_test'], data['y_test']

#数据预处理，对数据进行缩放
x_train, x_test = x_train / 255.0, x_test / 255.0


#搭建神经网络模型
model = models.Sequential()

#输入层 (Flatten 层)，将图像展平成 28*28 像素的特征矩阵
model.add(layers.Flatten(input_shape=(28, 28)))

#第一个隐藏层，设置激活函数为 ReLU，并添加 Dropout 层
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dropout(0.2))

#第二个隐藏层 (新增)，设置激活函数为 ReLU，并添加 Dropout 层
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dropout(0.2))

#第三个隐藏层 (新增)，设置激活函数为 ReLU，并添加 Dropout 层
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dropout(0.2))

#输出层，设置激活函数为 Softmax
model.add(layers.Dense(10, activation='softmax'))

#编译模型，设置损失函数、优化器、评估指标
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

#训练模型
history = model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))

#评估模型
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print('\nTest accuracy:', test_acc)


#绘制训练结果
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')
plt.show()


import cv2

#使用模型识别本地的某个图片的数字
img = cv2.imread('/Users/yclxiao/Downloads/num.png')
img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
img = cv2.resize(img, (28, 28))
img = img.reshape(1, 28, 28, 1)
img = img.astype('float32')
img /= 255
prediction = model.predict(img)
print(prediction)

```



### 6.3、股价预测

以上是图像识别案例，下面再来一个长序列案列，比如骨架预测。根据苹果最近几年的估计预测后续的股价。

```Python
import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras import layers

#下载股票数据（以苹果公司为例）
data = yf.download('AAPL', start='2020-01-01', end='2023-01-01')


#使用 "Close" 价格作为特征
data = data[['Close']]

#数据归一化
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)

#生成输入序列和目标序列
sequence_length = 60  # 使用前 60 天的数据预测下一天的价格
x_data = []
y_data = []

for i in range(sequence_length, len(scaled_data)):
    x_data.append(scaled_data[i-sequence_length:i, 0])
    y_data.append(scaled_data[i, 0])

x_data = np.array(x_data)
y_data = np.array(y_data)

#将数据划分为训练集和测试集
train_size = int(len(x_data) * 0.8)
x_train, x_test = x_data[:train_size], x_data[train_size:]
y_train, y_test = y_data[:train_size], y_data[train_size:]

#数据形状调整为 [样本数, 时间步数, 特征数]
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))



#预测
predicted_stock_price = model.predict(x_test)
predicted_stock_price = scaler.inverse_transform(predicted_stock_price)

#可视化结果
import matplotlib.pyplot as plt

real_stock_price = scaler.inverse_transform(y_test.reshape(-1, 1))

plt.plot(real_stock_price, color='red', label='Real AAPL Stock Price')
plt.plot(predicted_stock_price, color='blue', label='Predicted AAPL Stock Price')
plt.title('AAPL Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel('AAPL Stock Price')
plt.legend()
plt.show()



#保存data
data.to_csv('AAPL.csv')

```



## 7、总结

本篇主要聊了深度学习入门相关知识，旨在向开发者阐述深度学习的概念和常规开发步骤，现在的大模型都是基于深度学习训练而来，了解深度学习相关知识是非常有必要的。

代码地址：[https://github.com/yclxiao/developer_ai.git](https://github.com/yclxiao/developer_ai.git)

