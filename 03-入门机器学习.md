> 机器学习（Machine Learning，ML）是AI的一个分支，也是AI发展的基石。

## 1、开发者如何入门机器学习

本专栏是写给开发者的，用于快速了解AI相关知识，重在实践以及迅速开发出AI应用。所以，本篇所讲的机器学习的重点也是如何使用机器学习技术而非搞机器学习算法研究。

对于开发者而言，我们的价值是将机器学习应用到具体的业务场景中，所以在学习机器学习算法时，只需要了解原理即可。

可能刚开始接触的朋友，会有畏难心理，但是最好的办法就是行动起来，做好第一个Demo，就成功一大步了。

## 2、什么是机器学习

### 定义

一句话简述机器学习的定义：***让机器从数据集中学习，找到合适的函数，在用这个函数做预测。***

机器学习能够利用计算机的计算能力，从大量的数据中发现一个“函数”或者“模型”，并通过它来描述事物之间的关系，从而实现预测。

所以，机器学习最关键的步骤就是**根据已有数据，建立一个合适的“模型”**。然后，才能根据这个“模型”做预测。

**示例**

如下，展示一个机器学习的应用示例。

比如需求是：开发一个功能用来`预测家政阿姨的薪资`。我们需要给机器提供数据集，数据集里包含了的家政阿姨的“特征”（年龄、城市、获取的证书、工作年限、工作经历、菜系、技能等等）和 目前薪资。机器利用已有数据训练学习，得出一个“模型”。在根据这个“模型”和家政阿姨的“特征”，去预测大概薪资。

### 与计算机程序的区别

- **传统程序**：开发者根据已知规则，硬编码到程序里。相当于：开发者自己定义函数。
- **机器学习**：机器从已知数据集中，不断试错、优化，总结出规则。相当于：机器总结出函数。

## 3、开发工具

### Python

Python是人工智能领域的第一语言，也是运维领域非常趁手的脚本语言。如果不熟悉的Python的朋友，建议快速学习一下。

### Jupyter Notebook

Jupyter Notebook 是用于创建和共享程序和文档的 Web 应用程序。它是AI领域常用的开发和测试工具。

Jupyter Notebook 提供了一个环境，你可以在其中 编写代码，运行代码，查看结果，可视化数据，另外，还可以添加说明文本，文本支持数学公式和Markdown语法。并且可以将这些代码和文档与他人共享。

安装Jupyter Notebook虽说简单，但我还是觉得麻烦。为了更快进入开发和实践，我建议直接使用`VsCode 或 PyCharm`创建`ipynb`文件，然后选择对应的Python解释器，即可使用。

![](https://img.mangoant.top/blog/202408032004762.png)

### 常用库

机器学习常用的库有：

- Pandas：用来处理数据和分析数据。
- NumPy：用来处理多维数组，做大量的数学运算，做线性代数运算。
- Matplotlib：用来做数据可视化，可以创建静态、动态和交互式可视化图表。
- scikit-learn：提供简单高效的数据挖掘和数据分析工具，支持各种回归、分类和聚类算法。

如果还没安装的朋友，提前安装下这些组件。可以通过`pip install xxx`快速安装。

## 4、机器学习解决哪类问题

机器学习主要用来解决两类问题，回归问题 和 分类问题。

回归问题的标签（标签在下文有介绍）是连续数值。比如过去10年每个月的房价趋势，我们可以根据已知数据训练出一个模型，用来预测未来房价。回归问题的表现形式如下：

![](https://img.mangoant.top/blog/202408041706530.png)

分类问题的标签是离散性数值。比如，电商里的某个商品要不要推荐给用户 或者 判断某个邮件是否是垃圾邮件 或者 给商品分类等等。分类问题的表现形式如下：

![](https://img.mangoant.top/blog/202408041707270.png)

## 5、机器学习常用概念

### 特征和标签

机器学习的过程就是从数据中学习总结出函数。既然是函数，比如`y = a1 * x + b`，那总得有自变量和应变量。这些自变量x，就叫做**特征**（feature），因变量 y 叫做**标签**（label）。

### 数据集

一批历史特征和一批历史标签的集合，就是机器学习的**数据集**。

### 训练数据集

用来训练的数据集，就是**训练数据集**（training dataset）。

### 验证数据集

当机器通过训练找到了一个函数，还需要验证和评估这个函数的效果，这时候我们要给机器另一批同类数据特征，看机器能不能用这个函数预测出这批数据的标签。

这一过程就是在验证模型是否能被推广、泛化，而此时我们用到的数据集，就叫**验证数据集**（validation dataset）。有时，我们也会用测试数据集当做验证数据集。

### 算法

所谓**算法就是一套函数集，每个函数存在n个变量和参数**。

我们给机器建模的过程，就是从这套函数集里，寻找一个最合适的含有变量的函数，然后通过大量的数据训练去确定这个函数的每个参数，最终确定出一个函数。

比如`y = a1 * x + b`，就是一个函数。我们通过大量的数据训练，确定参数`a1`，然后在通过这个函数去做预测。

在学习算法时，千万不要陷入到算法的推演漩涡中，我们只需要了解原理和使用场景即可。

### 示例

以上面的`预测家政阿姨的薪资`需求为例，数据如下：

![](https://img.mangoant.top/blog/202408040717849.png)

表格里的蓝色部分是`特征`（代表了家政阿姨的特征），表格里的绿色表示`标签`（代表了阿姨的薪资），表格里的黄色表示`数据集`，一般会从数据集中选出10%~20%作为`验证数据集`。

## 6、机器学习分类

从以上的概念，我们大概猜到`标签`对于机器学习来说很重要，一批好的特征和标签，可以训练出好的模型。但是，机器学习也不一定要有标签，这跟机器学习的分类相关。

机器学习的常用分类有：监督学习、无监督学习、半监督学习、强化学习。目前，监督学习是应用最广泛的机器学习算法。

比如，你是一个水果商，想根据水果的特征（比如颜色、大小、形状）来判断它是什么水果。你可以用机器学习算法来帮你自动识别水果种类。以下以水果为例，通俗地解释下：

1. **监督学习**：
    - **概念**：你有一堆已经标记好种类的水果（训练数据有标签），并且你知道每个水果的特征（颜色、大小、形状）。你用这些数据来训练一个模型，让它学习如何根据特征判断水果种类。
    - **例子**：如果你有一堆苹果和橘子的照片，并且每张照片都标明了是苹果还是橘子，算法会学习这些照片的特征，然后能识别新照片中的水果是苹果还是橘子。
2. **无监督学习**：
    - **概念**：你有一堆水果，但没有标记它们的种类（即没有标签）。算法会自己找出水果之间的相似性和差异性，将它们进行分类，但是不知道分类的名字。
    - **例子**：你把一堆混合的水果丢给算法，它可能会发现苹果和橘子有不同的颜色和形状，并将它们分成不同的组，但机器不知道这些组具体叫什么名字。
3. **半监督学习：**
    - **概念**：你有一堆水果，有的做了标记，有的没有标记。即：在训练数据集中，有的数据有标签，有的数据没有标签。
4. **强化学习**：
    - **概念**：让机器基于环境而做出行动反应，这种算法是让机器通过**不断试错来达到最佳策略**的方式学习。就像教小孩骑自行车，小孩不断尝试，不断摔倒，逐渐学会保持平衡。所以，强化学习和监督学习的差异在于：监督学习是从数据中学习，而强化学习是从环境给它的奖惩中不断的试错学习。
    - **例子**：假设你有一个自动水果分类机器人，每次它正确分类一个水果就给它一个奖励。通过不断试错和奖励，它最终会学会如何更准确地分类水果。

## 7、常用算法

机器学习算法较多，下面介绍几个常用的：线性回归、决策树、支持向量机、k-近邻算法、随机森林。

在学习算法时，千万不要陷入到算法的推演漩涡中，我们只需要了解原理和使用场景即可。学习时，可以找一些简短视频配合一起学，效果会更好。

### 线性回归

- **用途**：用于连续数值的回归任务。
- **例子**：根据房子的面积和房间数量预测房价。
- **原理**：通过拟合一条直线来最小化预测值与真实值之间的均方误差，从而进行预测。
- **示例代码**：

```Python
from sklearn.linear_model import LinearRegression

X = [[1], [2], [3], [4]]
y = [1.5, 3.5, 3.0, 4.5]
clf = LinearRegression()
clf.fit(X, y)
print(clf.predict([[5]]))  # 输出：可能接近5
```

### **决策树**

- **用途**：用于分类任务。
- **例子**：根据水果的颜色、大小和形状一步步做决策，最后判断是苹果还是橘子。
- **原理**：通过递归地将数据集分割成更小的子集，直到每个子集中的样本属于同一个类别或者无法再分割。每个分割点（决策点）基于某个特征及其阈值。
- **示例代码**：

```Python
from sklearn.tree import DecisionTreeClassifier

X = [[0, 0], [1, 1]]
y = [0, 1]
clf = DecisionTreeClassifier()
clf.fit(X, y)
print(clf.predict([[2, 2]]))  # 输出：[1]
```

### **支持向量机**

- **用途**：用于分类任务。
- **例子**：将邮件分类为垃圾邮件和非垃圾邮件。
- **原理**：通过找到一个超平面将不同类别的数据点分开，同时最大化数据点到超平面的最小距离（即最大化间隔）。
- **示例代码**：

```Python
from sklearn import svm

X = [[0, 0], [1, 1]]
y = [0, 1]
clf = svm.SVC()
clf.fit(X, y)
print(clf.predict([[2, 2]]))  # 输出：[1]
```

### **k-均值聚类算法**

- **用途**：用于分类任务。
- **例子**：根据客户的购买行为、偏好等将客户分为不同的群体。
- **原理**：K均值是一种无监督学习算法，用于将数据集分成K个簇。算法通过迭代的方法，找到簇中心，并将数据点分配到最近的簇中心。
- **示例代码**：

```Python
from sklearn.cluster import KMeans
import numpy as np
  
#示例数据
X = np.array([[1, 2], [2, 3], [3, 4], [8, 7], [9, 8], [10, 9]])
  
#创建K均值模型
model = KMeans(n_clusters=2)
model.fit(X)
  
#预测簇标签
predicted_labels = model.predict([[1, 1], [9, 9]])
print(predicted_labels)
```

### **随机森林**

- **用途**：用于连续数值的任务。
- **例子**：根据多个决策树的投票结果预测股票的价格。
- **原理**：通过构建多个决策树（每棵树训练在数据的不同子集和特征的子集上），然后将这些树的结果进行平均或多数投票。
- **示例代码**：

```Python
from sklearn.ensemble import RandomForestClassifier

X = [[0, 0], [1, 1]]
y = [0, 1]
clf = RandomForestClassifier()
clf.fit(X, y)
print(clf.predict([[0.8, 0.8]]))  # 输出：[1]
```

## 8、动手实践-开发步骤和预处理

### 开发步骤

做机器学习项目，就是明确要解决的问题，再选定一个算法，然后对这个算法用数据进行训练，确定这个函数的各个参数，最终形成模型。

综上，一个机器学习项目从开始到结束大致分为 5 步，分别是：

1. 定义问题
2. 收集数据和预处理数据
3. 选择算法
4. 训练并确定出模型
5. 评估并优化模型

![](https://img.mangoant.top/blog/202408041903850.png)



### 如何定义问题

定义问题最重要的有2个步骤：

1. 分析业务场景和需求，搞清楚目标是什么。
2. 根据需求，选择合适的算法。比如是回归问题、还是分类问题。

### 如何预处理数据

一个好的模型离不开优质数据。预处理的过程就是对收集到的数据做清洗，然后构建特征集和标签集。

要确定好哪些因素影响到结果。比如，对家政阿姨做薪资预估，那么可能有年龄、工作年限、技能、证书、评价等维度会影响最终的薪资结果。那我们就要收集这些维度的数据，然后对数据做清晰，在构建特征集和标签集。

**数据收集**

收集数据有多种方式，比如：

- 业务代码里埋点采集
- 爬虫抓取
- 采集日志：在业务端通过异步写日志或者写消息中间件的方式保存数据。
- 数据汇总：比如业务数据分散在各个数据库中，通过大数据相关技术，将需要的数据合并到一处。

**数据清洗**

- 处理缺失数据
- 删除重复数据
- 处理异常数据

**构建特征集和标签集**

- 特征集：我们收集的影响业务结果的各个维度。
- 标签集：业务产生的结果值。

### 如何选择算法

选择算法这一过程基本是凭经验来。重点就是根据特征和标签之间的关系，选出一个合适的算法。

对于新手来说，我们只能凭直觉。如果你觉得特征数据和标签数据之间存在类似线性的关系，并且标签的值是连续的，那可以先选择线性回归算法试试。

如果标签的数据是离散型的，比如点击或者不点击、去或者不去，那么这类问题可以选择分类算法，比如先试试逻辑回归算法。

### 如何评估模型

一般在给定的数据集中，我们会拆分一部分作为测试集，用来验证模型的效果。在使用测试集进行模型效果评估时，可以使用机器学习工具包（如 scikit-learn）。

工具包中会提供常用的工具和指标，对测试集进行评估。比如 使用R2 或者 MSE 均方误差指标，来评估模型的好坏。一般来说，R2 的取值在 0 到 1 之间，R2 越大，说明拟合的模型越好。

#### R²（决定系数）

**定义**：R²就是决定系数，是指训练出的模型解释目标变量变异的比例，即正确预测了多少比例的数据，或者拟合程度如何。R²越接近1，表示模型对数据的拟合程度越好，即模型几乎正确预测了所有数据，R²越接近0，表示模型对数据的拟合程度越差，即模型几乎预测错了所有数据。

**注意：**R²值越高表示模型越好地拟合了数据，但并不意味着此模型是最好的选择。

**公式**：

![](https://img.mangoant.top/blog/202408061348846.png)

![](https://img.mangoant.top/blog/202408061351522.png)

#### MSE（均方误差）

**定义**：MSE是预测值与实际值之间误差的平方和的平均值，用于衡量模型预测的误差。MSE反映了预测值与实际值之间的平均平方误差，值越小，表示模型的预测误差越小。

**公式**：

![](https://img.mangoant.top/blog/202408061353040.png)

![](https://img.mangoant.top/blog/202408061353416.png)

### 循环迭代

按照上方的一轮步骤跑下来，可能得到的模型评估分数不理想，此时可能需要选择别的算法或者继续优化数据集，然后重新训练。

总之，寻求最优模型是一个持续循环迭代的过程。随着我们尝试的算法越来越多，数据集越来越多，模型会越来越好。



> 开发步骤定好之后，接下来动手实践2个小项目，运用下机器学习的5个步骤。

## 9、动手实践-回归分析

### 需求

我有一些家政行业的阿姨数据和合同数据，我想根据已有数据实现一个给家政阿姨预测薪资的功能。

### 实现

**1、定义问题**

根据以上需求，开始定义问题。

给定一些阿姨的特征和薪资的数据集。根据已知的家政阿姨的特征和薪资训练出模型，然后用模型预测某一个家政阿姨的薪资。

数据集中包含：家政阿姨的年龄、经验、每周工作小时数、技能等级、拥有的证书种类、薪资等等。示例如下：

![](https://img.mangoant.top/blog/202408041949398.png)

**2、收集数据和预处理数据**

这个示例里，我采用模拟生成数据的方式来演示：

```Python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

#创建模拟数据集
np.random.seed(42)
num_samples = 100

data = {
    'Age': np.random.randint(20, 60, size=num_samples),
    'Experience': np.random.randint(1, 20, size=num_samples),
    'Hours_Worked_Per_Week': np.random.randint(20, 60, size=num_samples),
    'Skill_Level': np.random.randint(1, 5, size=num_samples),
    'cert_kind': np.random.randint(1, 10, size=num_samples),
}

#假设薪资是由以下公式生成的，加上一些随机噪声
#薪资 = 500 + 年龄 * 10 + 经验 * 100 + 每周工作小时数 * 5 + 技能等级 * 200 + 证书种类 * 200 + 随机噪声
salary = (
    500 +
    data['Age'] * 10 +
    data['Experience'] * 400 +
    data['Hours_Worked_Per_Week'] * 5 +
    data['Skill_Level'] * 400 +
    data['cert_kind'] * 300 +
    np.random.normal(0, 1000, num_samples)  # 随机噪声
)

data['Salary'] = salary


```

**3、选择算法**

这里选择随机森林算法，定好森林中决策树的数量。在这里，将生成100棵决策树。如果数据多，可以提高到200以上。

```Python
#创建随机森林回归模型
model = RandomForestRegressor(n_estimators=100, random_state=42)
```

**4、训练并确定出模型**

训练前，先分离出特征数据和标签数据，在从数据集中选一部分作为测试集。然后开始训练。这里选择20%的数据作为测试集。

```Python
#分离特征和标签变量
X = df.drop(columns=['Salary'])
y = df['Salary']

#拆分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


#训练模型
model.fit(X_train, y_train)

#进行预测
y_pred = model.predict(X_test)
```

**5、评估并优化模型**

我们通过R²和MSE来评估模型的好坏。

```Python
#评估模型
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse:.2f}')

#可视化真实值与预测值
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, edgecolors=(0, 0, 0))
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)
plt.xlabel('Actual Salary')
plt.ylabel('Predicted Salary')
plt.title('Actual vs Predicted Salary')
plt.show()

```

结果如下，如果换成线性回归`LinearRegression`模型，则拟合的效果和均方误差更小。

```text
Mean Squared Error: 1384318.24
R^2 Score: 0.82
```

![](https://img.mangoant.top/blog/202408072307362.png)

## 10、动手实践-聚类分析

### 需求

我有一些鸢尾花的数据，包括：花萼长度、花萼宽度、花瓣长度、花瓣宽度、鸢尾花品种。我要实现一个根据鸢尾花的特性去判断鸢尾花品种的功能。

### 实现

**1、定义问题**

根据以上需求，开始定义问题。

以经典的鸢尾花分类为例，鸢尾花一共分3类，数据集中有一些鸢尾花的数据，可以根据不同的花萼长度、花萼宽度、花瓣长度、花瓣宽度，确定不同的鸢尾花品种。

数据集示例如下，我们需要训练出模型，然后给定一些花萼长度、花萼宽度、花瓣长度、花瓣宽度，让模型预测出鸢尾花品类。

![](https://img.mangoant.top/blog/202408041924793.png)

**2、收集数据和预处理数据**

这里直接使用经典鸢尾花分类的数据集。

```Python
from sklearn.datasets import load_iris

#加载数据集
iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['target'] = iris.target

#选择特征进行聚类
X = df.drop(columns=['target'])


```

**3、选择算法**

这里选择`K-means聚类`算法对数据做分类。

```Python
#进行K-means聚类
kmeans = KMeans(n_clusters=3, random_state=42)
```

**4、训练并确定出模型**

训练前，先选择特征进行聚类。然后开始训练。

```Python
kmeans.fit(X)

#获取聚类结果
df['cluster'] = kmeans.labels_

#将聚类标签转换为与真实标签相同的格式
df['cluster'] = df['cluster'].map({0:1, 1:0, 2:2})

```

**5、评估并优化模型**

对模型进行评估。

```Python
#计算聚类准确度
accuracy = accuracy_score(df['target'], df['cluster'])
print(f'Clustering Accuracy: {accuracy}')

#混淆矩阵
conf_matrix = confusion_matrix(df['target'], df['cluster'])
print('Confusion Matrix:')
print(conf_matrix)
```

结果如下，`accuracy_score` 的得分是 0.44 表示模型的准确率为 44%。这意味着模型在测试数据集上正确分类的样本占总样本数的比例为 44%。这是一个相对较低的准确率。

```text
Clustering Accuracy: 0.44
Confusion Matrix:
[[50  0  0]
 [ 0  2 48]
 [ 0 36 14]]
```

![](https://img.mangoant.top/blog/202408072335603.png)

## 11、动手实践-模型部署

机器学习的5个步骤执行完毕，模型就确定了，接下来就是保存模型和部署模型，让外部系统能够调用模型预测新的数据结果。

### 保存模型

模型说白了，就是一个包含了若干参数和变量的函数，在Python看来，模型就是对象。如果要保存对象，只需要序列化后存盘即可。

Python提供了多种工具来保存模型，比如`Joblib`。`Joblib` 会把内存中的模型对象做序列化处理，存储到硬盘上，一般存储模型时常用的文件名是`xxxxx.pkl`。当需要调用模型时，在用`Joblib` 将磁盘中的`xxxxx.pkl`反序列化成 Python 对象，存在内存中。

```Python
import joblib

#保存模型
joblib.dump(model, 'salary_predictor_model.pkl')

#加载模型
model = joblib.load('salary_predictor_model.pkl')
```

### 部署模型

部署机器学习模型有多种方式，一般是通过 Web 应用把模型部署到 Web 服务器，供外部系统调用。

一般用 FastAPI框架（或者Flask 框架）开发一个轻量级的 Web 应用。在Web应用里，使用`Joblib`加载模型，然后在把需要预测的数据丢给模型处理。

使用示例如下：

```Python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import joblib
import numpy as np

#创建FastAPI应用
app = FastAPI()

#加载训练好的模型
model = joblib.load("salary_predictor_model.pkl")


#定义输入数据模型
class Features(BaseModel):
    age: int
    experience: int
    hours_Worked_Per_Week: int
    skill_Level: int
    cert_kind: int


#定义预测请求的路由
@app.post("/predict")
async def predict(features: Features):
    try:
        #将输入数据转换为模型的输入格式
        input_data = np.array(
            [
                [
                    features.age,
                    features.experience,
                    features.hours_Worked_Per_Week,
                    features.skill_Level,
                    features.cert_kind,
                ]
            ]
        )

        #使用模型进行预测
        prediction = model.predict(input_data)

        return {"predicted_salary": prediction[0]}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


#启动服务器
if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8990)


```

```Bash
#启动web应用
uvicorn predictor:app --host 0.0.0.0 --port 8990

#访问接口地址
http://localhost:8990/docs
```

![](https://img.mangoant.top/blog/202408080918499.png)

## 12、总结

这篇文章主要介绍了机器学习入门的相关知识，旨在向开发者阐述机器学习的概念和常规开发步骤。这将为后续了解GPT等大语言模型 和 开发AI应用 奠定坚实的基础。

至此，机器学习的入门知识已梳理完毕了，接下来就是实践了！！！

代码地址：[https://github.com/yclxiao/developer_ai.git](https://github.com/yclxiao/developer_ai.git)

